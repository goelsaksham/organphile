{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Model\n",
    "\n",
    "This notebook is used to train a LSTM RNN model for All class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "This section will import the required libaries that will be used to actually implement the training for the Vanilla RNN Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding this so that can use the python scripts for loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the python script training data loader function. This function loads the data from the *.wav* files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_scripts.directory_funcs import *\n",
    "from py_scripts.wav_file_funcs import *\n",
    "from py_scripts.misc_audio_signal_funcs import *\n",
    "from py_scripts.raw_training_data_creation import load_irmas_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data\n",
    "\n",
    "This section uses the script from the training data creation module to load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cel', 'cla', 'flu', 'gac', 'gel', 'org', 'pia', 'sax', 'tru', 'vio', 'voi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First define the training data directory, and find the different classes that exist\n",
    "training_data_dir = '../../data/whole_dataset/training/'\n",
    "get_subdirectory_names(training_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes that we will work\n",
    "all_classes = get_subdirectory_names(training_data_dir)\n",
    "# Defining the classes that will be the ones on which we will train for the project\n",
    "classes_for_project = ['gel', 'pia', 'sax', 'vio', 'voi']\n",
    "mapping_to_index = dict(zip(classes_for_project, range(len(classes_for_project))))\n",
    "# Getting the frequency for each class\n",
    "class_num_files = dict(zip(classes_for_project, [len(get_file_names(construct_path(training_data_dir, class_name), '*.wav')) for class_name in classes_for_project]))\n",
    "# Defining the class which will be used as the one v/s all classifier to denote all other classes except the current class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Data from gel\n",
      "Processing: 50 files\n",
      "Loaded all the data from the class\n",
      "Getting Data from pia\n",
      "Processing: 50 files\n",
      "Loaded all the data from the class\n",
      "Getting Data from sax\n",
      "Processing: 50 files\n",
      "Loaded all the data from the class\n",
      "Getting Data from vio\n",
      "Processing: 50 files\n",
      "Loaded all the data from the class\n",
      "Getting Data from voi\n",
      "Processing: 50 files\n",
      "Loaded all the data from the class\n"
     ]
    }
   ],
   "source": [
    "# Defining the various parameters for loading the input data\n",
    "rnn_window = (300, 300) # in the format of length of vector and the shift\n",
    "class_num_examples = 50\n",
    "X, y = load_irmas_data(training_data_dir, classes_for_project, rnn_window, number_of_training_examples_per_class=class_num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 294, 300), (1000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the shape\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeing up previous memory\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Training Samples: (800,)\n",
      "Total number of timestamp values for each sample: 294\n",
      "Total number of features for each sample: 300\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Number of Training Samples: {y_train.shape}')\n",
    "print(f'Total number of timestamp values for each sample: {X_train.shape[1]}')\n",
    "print(f'Total number of features for each sample: {X_train.shape[-1]}')\n",
    "#print(f'Minimum Feature Value: {np.min(X_train)}, Maximum Feature Value: {np.max(X_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Testing Samples: (200,)\n",
      "Total number of timestamp values for each sample: 294\n",
      "Total number of features for each sample: 300\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Number of Testing Samples: {y_test.shape}')\n",
    "print(f'Total number of timestamp values for each sample: {X_test.shape[1]}')\n",
    "print(f'Total number of features for each sample: {X_test.shape[-1]}')\n",
    "#print(f'Minimum Feature Value: {np.min(X_train)}, Maximum Feature Value: {np.max(X_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gel', 'pia', 'sax', 'vio', 'voi'], dtype='object') Index(['gel', 'pia', 'sax', 'vio', 'voi'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y_train_categorical = pd.Categorical(y_train)\n",
    "y_train_numerical = y_train_categorical.codes\n",
    "y_test_categorical = pd.Categorical(y_test)\n",
    "y_test_numerical = y_test_categorical.codes\n",
    "# Checking the categries\n",
    "print(y_train_categorical.categories, y_test_categorical.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of categories\n",
    "len(y_train_categorical.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training for more than 1 class then need to convert to categorical\n",
    "if len(y_train_categorical.categories) > 2:\n",
    "    y_train_numerical = to_categorical(y_train_numerical)\n",
    "    y_test_numerical = to_categorical(y_test_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_numerical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check For Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X_train', 282240128),\n",
       " ('X_test', 70560128),\n",
       " ('y_train', 9696),\n",
       " ('y_test', 2496),\n",
       " ('y_train_categorical', 1284),\n",
       " ('Dense', 1056),\n",
       " ('EarlyStopping', 1056),\n",
       " ('Embedding', 1056),\n",
       " ('LSTM', 1056),\n",
       " ('ModelCheckpoint', 1056),\n",
       " ('Sequential', 1056),\n",
       " ('TensorBoard', 1056),\n",
       " ('y_test_categorical', 684),\n",
       " ('nb_dir', 285),\n",
       " ('class_num_files', 240),\n",
       " ('mapping_to_index', 240),\n",
       " ('all_classes', 160),\n",
       " ('training_data_dir', 153),\n",
       " ('Input', 136),\n",
       " ('check_output_directory', 136),\n",
       " ('construct_path', 136),\n",
       " ('exist_directory', 136),\n",
       " ('exist_file', 136),\n",
       " ('get_directory_contents', 136),\n",
       " ('get_file_names', 136),\n",
       " ('get_left_channel_data', 136),\n",
       " ('get_right_channel_data', 136),\n",
       " ('get_sound_signals', 136),\n",
       " ('get_subdirectory_names', 136),\n",
       " ('load_irmas_data', 136),\n",
       " ('normalize_sound_signals', 136),\n",
       " ('read_wav_file', 136),\n",
       " ('shift_sound_signals', 136),\n",
       " ('to_categorical', 136),\n",
       " ('train_test_split', 136),\n",
       " ('y_test_numerical', 112),\n",
       " ('y_train_numerical', 112),\n",
       " ('classes_for_project', 104),\n",
       " ('np', 80),\n",
       " ('pd', 80),\n",
       " ('sequence', 80),\n",
       " ('wavfile', 80),\n",
       " ('rnn_window', 64),\n",
       " ('class_num_examples', 28),\n",
       " ('X', 16),\n",
       " ('y', 16)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "This section will define the model architecture that will be used for the training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features (Feature Vector Length): 300, Number of Time Stamps: 294\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameters for the Embedding layer\n",
    "number_of_features = X_train.shape[-1]\n",
    "number_of_time_stamps = X_train.shape[1]\n",
    "print(f'Number of Features (Feature Vector Length): {number_of_features}, Number of Time Stamps: {number_of_time_stamps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 294, 50)           70200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 294, 5)            255       \n",
      "=================================================================\n",
      "Total params: 70,455\n",
      "Trainable params: 70,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "rnn_layer_num_units = 50\n",
    "num_classes_for_training = len(y_train_categorical.categories)\n",
    "model = Sequential()\n",
    "model.add(LSTM(rnn_layer_num_units, input_shape=(number_of_time_stamps, number_of_features), dropout = 0.1, \n",
    "               return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(num_classes_for_training, activation='softmax')))\n",
    "# Compiling the model\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a checkpoint\n",
    "parent_weight_save_dir = '../../data/Training Results/BLSTM/Weights'\n",
    "tensor_board_dir_path = '../../data/Training Results/BLSTM/TensorBoard'\n",
    "check_output_directory(parent_weight_save_dir)\n",
    "current_experiment_name = f'AllClasses_InputVectorLen-{number_of_features}_TimeStamps-{number_of_time_stamps}_CT-{time.time()}'\n",
    "weight_file_path = os.path.join(parent_weight_save_dir, f'{current_experiment_name}.hdf5')\n",
    "tensor_board_file_path = os.path.join(tensor_board_dir_path, current_experiment_name)\n",
    "check_output_directory(tensor_board_file_path)\n",
    "checkpoint = ModelCheckpoint(weight_file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir=tensor_board_file_path)\n",
    "early_stopping_criteria = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "callbacks_list = [tensorboard, checkpoint, early_stopping_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_2 to have 3 dimensions, but got array with shape (800, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f4ca219a4619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_numerical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected time_distributed_2 to have 3 dimensions, but got array with shape (800, 5)"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_numerical, epochs=50, batch_size=64, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test_numerical, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "rnn_layer_num_units = 100\n",
    "num_classes_for_training = len(y_train_categorical.categories)\n",
    "model = Sequential()\n",
    "model.add(LSTM(rnn_layer_num_units, input_shape=(number_of_time_stamps, number_of_features), dropout = 0.1))\n",
    "model.add(Dense(num_classes_for_training, activation='softmax'))\n",
    "# Compiling the model\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a checkpoint\n",
    "parent_weight_save_dir = '../../data/Training Results/LSTM/Weights'\n",
    "tensor_board_dir_path = '../../data/Training Results/LSTM/TensorBoard'\n",
    "check_output_directory(parent_weight_save_dir)\n",
    "current_experiment_name = f'AllClasses_InputVectorLen-{number_of_features}_TimeStamps-{number_of_time_stamps}_CT-{time.time()}'\n",
    "weight_file_path = os.path.join(parent_weight_save_dir, f'{current_experiment_name}.hdf5')\n",
    "tensor_board_file_path = os.path.join(tensor_board_dir_path, current_experiment_name)\n",
    "check_output_directory(tensor_board_file_path)\n",
    "checkpoint = ModelCheckpoint(weight_file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir=tensor_board_file_path)\n",
    "early_stopping_criteria = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "callbacks_list = [tensorboard, checkpoint, early_stopping_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_numerical, epochs=50, batch_size=64, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test_numerical, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "rnn_layer_num_units = 50\n",
    "num_classes_for_training = len(y_train_categorical.categories)\n",
    "model = Sequential()\n",
    "model.add(LSTM(rnn_layer_num_units, input_shape=(number_of_time_stamps, number_of_features), dropout = 0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(num_classes_for_training, activation='softmax'))\n",
    "# Compiling the model\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a checkpoint\n",
    "parent_weight_save_dir = '../../data/Training Results/LSTM/Weights'\n",
    "tensor_board_dir_path = '../../data/Training Results/LSTM/TensorBoard'\n",
    "check_output_directory(parent_weight_save_dir)\n",
    "current_experiment_name = f'AllClasses_InputVectorLen-{number_of_features}_Dense20_TimeStamps-{number_of_time_stamps}_CT-{time.time()}'\n",
    "weight_file_path = os.path.join(parent_weight_save_dir, f'{current_experiment_name}.hdf5')\n",
    "tensor_board_file_path = os.path.join(tensor_board_dir_path, current_experiment_name)\n",
    "check_output_directory(tensor_board_file_path)\n",
    "checkpoint = ModelCheckpoint(weight_file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir=tensor_board_file_path)\n",
    "early_stopping_criteria = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "callbacks_list = [tensorboard, checkpoint, early_stopping_criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_numerical, epochs=50, batch_size=64, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test_numerical, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
